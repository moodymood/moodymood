<!-- Guessability -->
<div class="modal-content">            
    <div class="close-modal" data-dismiss="modal">
        <div class="lr">
            <div class="rl">
            </div>
        </div>
    </div>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <div class="modal-body">
                    <h2>User-defined gestures for Agumented Reality with Smart Phones.</h2>
                    <hr class="star-primary">
                    <img src="img/portfolio/Guessability.png" class="img-responsive img-centered" alt="">
                    <div class="text-justify">
                        
                    
                    <p><i>Augmented reality</i> (AR) is a promising technology which opens a universe of new possibilities. However, when introducing new technologies, the first step should be understanding the effect on user interaction. 
                    </p>
                    <p>Therefore I performed a guessability study to elicit end-user gestures for selection, zoom and rotation tasks with mobile phones on 15 participants.
                    Findings from the experiment allowed to define a gestures taxonomy for AR reality environments</p>
                    <p>Results have been published in the form of a short abstract presented at Computer Graphics & Visual Computing (CGVC) 2015 @UCL, London.</p>
                    
                    <p class="text-right"><i class="fa fa-paperclip" aria-hidden="true"></i> <small><a target="_blank" href="pdf/portfolio/fmadeddu-CGVC2015-fin.pdf"> CGVC Short Abtract <i class="fa fa-file-pdf-o"></i> </a>, <a target="_blank" href="pdf/portfolio/fmadeddu-CGVC2015-ppt.pdf"> CGVC Presentation <i class="fa fa-file-powerpoint-o"></i></a>, <a target="_blank" href="pdf/portfolio/Guess-AR-gestures.pdf"> Full Study Report<i class="fa fa-file-pdf-o"></i> </a></small></p>
                        
                    <p class="text-right"><i class="fa fa-link" aria-hidden="true"></i> <small><a target="_blank" href="http://cgvc15.cs.ucl.ac.uk/programme.html"> CGVC 2015 Website <i class="fa fa-globe" aria-hidden="true"></i></a></small></p>
                   
                    <h3>The Guessability Study.</h3>
                    
                    <br/>
                        
                    <h4>Users.</h4>   
                        
                    <div class="text-center"><i class="fa fa-male" aria-hidden="true"></i><i class="fa fa-male" aria-hidden="true"></i><i class="fa fa-female" aria-hidden="true"></i></div>
                       
                    <p>The study involved 15 participants, mostly men: 60% of male and 40% female. The age range was 19-40. </p>
                      
                    <div class="text-center"><i class="fa fa-graduation-cap" aria-hidden="true"></i></div>
                       
                    <p>93% of participants were students, of whom 73% from the computer science department, hence with a strong technical background.</p>
                   
                    <div class="text-center"><i class="fa fa-laptop" aria-hidden="true"></i></div>
                        
                    <p>All students included themselves in a range from normal to expert mobile user and all of them declared to own at least one mobile device. The same also declared that, when using mobile devices, they were usually able to think of the action which is needed to achieve a certain goal and to understand the given feedback, concepts formalised by Donald Norman as execution and evaluation gulfs.
                   
                    <div class="text-center">[<i class="fa fa-picture-o" aria-hidden="true"></i>]</div>
                    
                    <p>Even if the 40% knew what AR was, only the 26% had tried an AR application.</p>
    
                    <br/>

                    <h4>Apparatus.</h4>
                        
                    <div class="text-center"><i class="fa fa-mobile" aria-hidden="true"></i></div>
                    <p>I developed a custom application to let the user interact with objects in an AR environment and to record this interaction. The smartphone used to deploy the custom application was a Samsung Galaxy Note 3, measuring 151.2 mm x 79.2 mm x 8.3 mm with a 1080 x 1920 of resolution.</p>
                    <p> For each surface gesture, the application:</p>
                        <ul>
                            <li><p>captured the coordinates of the user’s fingers and saved then as images, where the colour of each line represents a different finger; </p></li>
                            <li><p>a screenshot was captured to contextualise the gesture.</p></li> 
                        </ul>
                    <p>The application was developed to give no feedback, as the expected feedback was part of the information I wanted to elicit from participants.</p>
                        
                    <div class="row">
                        
                        <div class="col-lg-4  col-sm-4"><img class="img-responsive img-rounded" src="img/portfolio/Guessability_samsung.png" alt="">
                        </div>

                        <div class="col-lg-4  col-sm-4"><img class="img-responsive img-rounded" src="img/portfolio/Guessability_interaction.png" alt="">
                        </div> 
                        
                        <div class="col-lg-4  col-sm-4"><img class="img-responsive img-rounded" src="img/portfolio/Guessability_interaction-out.png" alt="">
                        </div> 
                    </div>
                        
                    <p class="text-right"><i class="fa fa-link" aria-hidden="true"></i> <small><a target="_blank" href="https://github.com/moodymood/MyEgypt"> Custom App Source Code <i class="fa fa-github" aria-hidden="true"></i></a></small></p>
                        
                    <div class="text-center"><i class="fa fa-cube" aria-hidden="true"></i></div>
                    <p>The target object was a rectangular box with an image on top, a 5£ note. This image, once captured by the camera, triggered the AR display.</p>
                    <p>The choice of using a box, rather than a sheet of paper, improved the interactive affordance of the object, so that the participant was implicitly made aware that they could touch the object. In this way, the bias of surface and motion gestures over direct manipulation was mitigated.</p>
                        
                    <br/>
                        
                    <h4>Data gathering.</h4>
                    
                    <div class="text-center"><i class="fa fa-list" aria-hidden="true"></i></div>
                    <p>An initial structured questionnaire was used to gather demographic information and technological knowledge towards mobile technologies and AR.
                    <br/>
                    During the experiment I also used a structured data sheet to jot down observations about the participant’s interaction with the smart phone.</p> 
                        
                    <p class="text-right"><i class="fa fa-paperclip" aria-hidden="true"></i> <small><a target="_blank" href="pdf/portfolio/Guessability_questionnarie.pdf"> Questionnaire <i class="fa fa-file-pdf-o"></i></a>, <a target="_blank" href="pdf/portfolio/Guessability_note.pdf"> Data Sheet <i class="fa fa-file-pdf-o"></i> </a></small></p>
                    
                    <div class="text-center"><i class="fa fa-microphone" aria-hidden="true"></i></div>
                    <p>The interactions were also partially recorded by the preinstalled logging application.
                    <br/>
                    A semi-structured interview was conducted to gather information about the participants’ opinion on the performed gestures. 
                    <br/>
                    All stages of the experiment were audio-recorded and participants were asked to adopt a talk-aloud protocol.</p>
                        
                    <p class="text-right"><i class="fa fa-paperclip" aria-hidden="true"></i> <small><a target="_blank" href="pdf/portfolio/Guessability_interview.pdf"> Interview <i class="fa fa-file-pdf-o"></i> </a></small></p>
                        
                    <br/>
                        
                    <h4>Tasks.</h4>
                        
                    <p>Participants were asked to complete four tasks. Given a scene displayed by the smart phone’s camera, a 3D model of an Anubis statue and a bag appeared, and participants were asked to perform the action they felt was most appropriate to cause a specific effect.</p>
                    <ol>
                        <li><p><i>Select</i> the Anubis statue and dragging it inside the bag</p></li>
                        <li><p><i>Zoom in/out</i> the Anubis statue</p></li>
                        <li><p><i>Rotate</i> the Anubis statue</p></li>
                        <li><p>Free exploration of the scene.</p></li>
                    </ol> 
                    <p>To avoid bias in the explanation of the task, words focused on the effect to achieve rather than on the action needed to obtain it. So for example, instead of asking the participant to rotate the Anubis model, they were asked how they would see the other side of the model.</p>
                    
                    <p class="text-right"><i class="fa fa-paperclip" aria-hidden="true"></i> <small><a target="_blank" href="pdf/portfolio/Guessability_task.pdf"> Task Description <i class="fa fa-file-pdf-o"></i> </a></small></p>
                                       
                    <br/>
                        
                    <h4>Procedure.</h4>
                        
                    <p> Participants were given a task description paper to introduce them to the AR concept and the experimental procedure. Once the participant finished reading, they were given a questionnaire to complete. After these instructions, the questionnaire was completed and the participant was given a smart phone and asked to perform the four tasks. As target recognition led outside the scope of the experiment,  how to trigger the  AR layer appearance was explained. Finally, an interview was conducted.</p>
                    <p>The experiment lasted 15 to 30 minutes and the participant sat next to me in front of a rectangular desk. The target object was positioned in front of us at a reachable distance.</p>
                        
                    <div class="row">
                        <div class="col-lg-4  col-sm-4"><img class="img-responsive img-rounded" src="img/portfolio/Guessability_env.png" alt="">
                        </div> 
                        
                        <div class="col-lg-4  col-sm-4"><img class="img-responsive img-rounded" src="img/portfolio/Guessability.png" alt="">
                        </div>

                        <div class="col-lg-4  col-sm-4"><img class="img-responsive img-rounded" src="img/portfolio/Guessability_gesture.png" alt="">
                        </div>
                    </div>
                    <div class="row">
                        
                        <div class="col-lg-4  col-sm-4"><img class="img-responsive img-rounded" src="img/portfolio/Guessasbility_target.png" alt="">
                        </div>

                        <div class="col-lg-4  col-sm-4"><img class="img-responsive img-rounded" src="img/portfolio/Guessasbility_anubis1.png" alt="">
                        </div> 
                        
                        <div class="col-lg-4  col-sm-4"><img class="img-responsive img-rounded" src="img/portfolio/Guessasbility_anubis2.png" alt="">
                        </div> 
                    </div>
                        
                    <br/>
                        
                    <h4>Results.</h4>
                    
                    <br/>
 
                    <h5>Elicited Gestures.</h5>
                           
                    <p>Results show that, given a specific category, the choice of a gesture was straightforward. In particular, for the selection task, 93% of the participants chose a surface drag and drop gesture. For the zoom task, even if the agreement was lower, more than a half (53%) chose the pinch to zoom surface gesture. Finally, for the rotation task, this trend was inverted and the highest agreement was towards a direct manipulation, meaning participants directly rotated the target object. However, the agreement for the gesture was the lowest of the three tasks. In fact, only 40% of the participants tried the gesture first.</p>
                         
                    <div class="row">
                        <div class="col-lg-4  col-offset-4"></div>
                        <div class="col-lg-4  col-sm-4"><img class="img-responsive" src="img/portfolio/Guessability_graph1.png" alt="">
                        </div>    
                    </div>
                          
                    <br/>
                        
                    <h5>First attempt, Standard and Preferred Gestures</h5>
                    
                    <p>During the experiment an additional gesture classification emerged. 
                    <br/>
                    When the participant was given with the phone and asked to guess the right action to cause a specific effect, he performed his <b>first attempt gesture </b>, so the first gesture which came to his mind. However it frequently happened that the participant did not stop trying, gradually discovering new gestures which, in some cases, were defined as more satisfactory than the first one, hence defined as <b>preferred gestures</b>. Finally, the <b>standard gesture</b> was the one that, according to the participant opinion, most of people would have tried when asked to achieve a specific task. 
                    </p>
                    <p>For example, for the zoom task, several participants tried to move the phone close the Anubis as first attempt gesture, and they recognised it as their favourite; however, when they were asked about their opinion on which kind of gesture other people would have done, some of them chose the pinch to zoom surface gesture.</p>
                    
                    <div class="row">
                        <div class="col-lg-4  col-sm-4"><img class="img-responsive" src="img/portfolio/Guessability_selection-table.png" alt="">
                        </div>
                        
                        <div class="col-lg-4  col-sm-4"><img class="img-responsive" src="img/portfolio/Guessability_rotation-table.png" alt="">
                        </div>
                        
                        <div class="col-lg-4  col-sm-4"><img class="img-responsive" src="img/portfolio/Guessability_zoom-table.png" alt="">
                        </div>
                    </div>    
                        
                    <br/>
                        
                    <h4>Analysis.</h4>
                    
                    <br/>
                    
                    <h5> Past Experience vs AR Effect.</h5>
                        
                    <p> In general it was possible to different group of participant which approached the interaction in a different way. For a group of participants, the interaction seemed to be strongly influenced by <b>their background</b>. Therefore, because touch is the primarymobile phone modality, they try to interact with the scene through surface gestures: in particular they applied the most common gestures, such as the drag and drop and the pinch to zoom, to the AR environment.
                    <br/>
                    For example one of the participant, when asked if it was easy to think about a gesture to achieve the selection task answered:</p>
                    <blockquote>Yes, it was easy but I always actually think of the previous experience [...] it’s like simulate what we are doing when we use a computer when you put a file into a folder and you drag one into each other.</blockquote>
                    <p>In some cases participants expressed the concern that interacting with the target object could be complicated: one participant mentioned the possibility that the target object could have been fixed somewhere, while another was concerned that the target object could have been too heavy to be moved.</p>
                    <p>However it has to be noted that in the environment settings the target object was close to the participant, light and easy to manipulate.</p>
                    <p>
                    All these observations provide an explanation for the preference towards surface gestures over the others.
                    Moreover some people stated to prefer surface gestures because, being the mobile phone already in their hands, were more immediate (assumption confirmed by the Fitt’s law).</p>
                        
                    <p>For the other group of participants, even if they recognised that it was easy to think about surface gestures to achieve the given tasks, movement and direct manipulation gestures were preferred. The given reason was that they perceived the <b>AR as equal to the reality</b>, therefore they felt that the interaction should have been as real as possible. 
                    <br/>
                    For example one of the participant stated:</p>
                    <blockquote> I like the idea of augmented reality and I think that the interaction should be augmented based [...] (to interact) shouldn’t be difficult because it is the same of the real world [...] I think this is the point of augmented reality.</blockquote>

                    <p>The contrast between the two approaches emerged with the rotation task. As a participant stated:</p>
                    <blockquote>The zoom and the selection, it’s something we are used to every day, the zoom with the camera for example, but the rotation is not so common so maybe other people will try different gestures.</blockquote>
                    
                    <br/>
                    
                    <h5>Needs of consistency, coherence and control.</h5>
                        
                    <p>In general, once that the scene was accepted as real, <b>consistency</b> and <b>coherence</b> were required. For example a participant tried for few minutes to move Anubis around the table to see if the lighting of the object was consistent with the lighting of the room.
                    <br>
                    The same participant also moved a pen first between the camera and the target object, then behind the target object to check, again, if the consistency hold. Another participant instead stated that he would have expect Anubis to fall down from the rectangular box if he tried to till it.</p>
                    <p>Another interesting observation was that, while when interacting trough common gestures (pinch to zoom, drag and drop) participants seems to suspend they critical judgment, they look for coherence and consistency when they had to think about a custom gesture. For example, participant weren’t disturbed by the fact that pinch to zoom and drag and drop were 2D gestures, but caused a 3D effect; however when trying to achieve the rotation task with surface gestures, most of participants underlined the incongruence.</p>
                    <p>Along with the desire of consistency and coherence, participants often expressed the of <b>control</b>. For example several participants stated that they would have liked to be able to make the augmented object disappear once they had enough of it.</p>
                    <p>These findings are not surprising, in fact control and consistency are basic usability requirement suggest by both the Nielsen’s 10 Heuristics and the 8 Schneiderman’s Golden rules.</p>
                    
                    <br/>
                        
                    <h5>Abstract and Concrete gestures</h5>
                        
                    <p>A further classification on gestures can be made according to their <b>abstract</b> or <b>concrete</b> nature. For example, shaking the phone to achieve selection is an abstract gesture as the relationship cause-effect is not clear whereas moving the phone around the object or rotating the target object itself, are a concrete gestures, in the sense they emulates the action we would perform in a real world.</p>

                    <p>Some elicited gestures caused a <b>conflict</b>. For example, the double tap was used both to achieve zoom but also selection.
                    This could suggest that, when a gesture does not share a high agreement, it is better to provide the user with extra information, to solve the ambiguity.</p>
                        
                    <p>During the free exploration task several participants pointed out that they would have expected other GUI elements to appear to help them interact with the scene. 
                    <br/>
                    For example, for the zoom task often it was hypothesised a plus or minus button for the zoom-in and zoom-out while for the rotation task it was often suggested the visualisation of the dimensional axis for a manipulation of the object.</p>
                        
                    <br/>
                        
                    <h3>Take home.</h3>
                        
                    <p>Participant behaviour seemed highly influenced by previous experience, especially when a standard gesture existed for the task. In these cases, the fact that they were interacting with a AR environment did not affect the gesture for the task. However, when a standard gesture did not exist, as for the case of the rotation task, their behaviour was affected by their critical judgment. Moreover, the current research confirmed a preference for simple gestures. In fact participants always interacted with the screen using one or two fingers. In general, there was a difference among the first participant’s gesture, the gesture recognised as standard, and the preferred gesture.</p>
                       
                    <br/>
                        
                    <h3>Next.</h3>
                        
                    <p>The next step will be to perform an evaluation against the elicited gestures.</p>
                    
                    <br/>
                                        
                    </div>

                    <ul class="list-inline item-details">
                        <li>Client:
                            <strong><a target="_blank" href="http://www.swansea.ac.uk/">Swansea University</a>
                            </strong>
                        </li>
                        <li>Date:
                            <strong><a href="#">April 2015</a>
                            </strong>
                        </li>
                        <li>Technologies:
                            <strong><a href="#">User Research, Guessability Study, Augmented Reality, Mobile Intraction, Unity 3d, C#, Lab Study</a>
                            </strong>
                        </li>
                    </ul>
                    <button type="button" class="btn btn-default" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                </div>
            </div>
        </div>
    </div>
</div>